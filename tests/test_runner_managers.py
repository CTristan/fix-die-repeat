"""Tests for runner manager classes."""

from pathlib import Path
from unittest.mock import MagicMock, patch

from fix_die_repeat.config import Paths, Settings
from fix_die_repeat.runner_artifacts import ArtifactManager
from fix_die_repeat.runner_pr import PrInfo, PrReviewManager
from fix_die_repeat.runner_review import ReviewManager
from fix_die_repeat.utils import get_file_line_count, get_git_revision_hash

FILTER_LOG_LINE_COUNT = 400
FILTERED_LOG_MAX_LINES = 300
REGULAR_COMPACT_LINES = 50
EMERGENCY_COMPACT_LINES = 100
RESOLVE_THREAD_CALL_COUNT = 2


class TestArtifactManager:
    """Tests for ArtifactManager behaviors."""

    def test_filter_checks_log_large_file(self, tmp_path: Path) -> None:
        """Filter large check logs to include errors and tail."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = ArtifactManager(settings, paths, logger)

        lines = [f"line {idx}" for idx in range(FILTER_LOG_LINE_COUNT)]
        lines[200] = "ERROR: failing build"
        paths.checks_log.write_text("\n".join(lines))

        manager.filter_checks_log()

        filtered = paths.checks_filtered_log.read_text()
        assert filtered.startswith("=== FILTERED CHECK OUTPUT")
        assert "ERROR: failing build" in filtered
        assert len(filtered.splitlines()) <= FILTERED_LOG_MAX_LINES

    def test_check_oscillation_detects_repeat(self, tmp_path: Path) -> None:
        """Detect repeated check output hashes."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = ArtifactManager(settings, paths, logger)

        paths.checks_log.write_text("identical output")
        current_hash = get_git_revision_hash(paths.checks_log)
        paths.checks_hash_file.write_text(f"{current_hash}:1\n")

        warning = manager.check_oscillation(iteration=2)

        assert warning is not None
        assert "iteration 1" in warning

    def test_check_and_compact_artifacts_regular(self, tmp_path: Path) -> None:
        """Compact artifacts when over regular threshold."""
        settings = Settings()
        settings.compact_threshold_lines = 10
        settings.emergency_threshold_lines = 200
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = ArtifactManager(settings, paths, logger)

        paths.review_file.write_text("\n".join(["line"] * 75))

        result = manager.check_and_compact_artifacts()

        assert result is True
        assert get_file_line_count(paths.review_file) == REGULAR_COMPACT_LINES

    def test_emergency_compact_truncates_files(self, tmp_path: Path) -> None:
        """Emergency compaction truncates large artifacts."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = ArtifactManager(settings, paths, logger)

        paths.review_file.write_text("\n".join(["line"] * 150))
        paths.build_history_file.write_text("\n".join(["line"] * 150))

        manager.emergency_compact()

        assert get_file_line_count(paths.review_file) == EMERGENCY_COMPACT_LINES
        assert get_file_line_count(paths.build_history_file) == EMERGENCY_COMPACT_LINES


class TestReviewManager:
    """Tests for ReviewManager behaviors."""

    def test_build_review_prompt_push_mode(self, tmp_path: Path) -> None:
        """Attach diff when under threshold."""
        settings = Settings(auto_attach_threshold=5000)
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        paths.diff_file.write_text("diff")
        logger = MagicMock()
        manager = ReviewManager(settings, paths, tmp_path, logger)

        pi_args: list[str] = []
        prompt = manager.build_review_prompt(diff_size=20, pi_args=pi_args)

        assert "changes.diff" in prompt
        assert pi_args == [f"@{paths.diff_file}"]

    def test_build_review_prompt_pull_mode(self, tmp_path: Path) -> None:
        """Skip diff attachment when over threshold."""
        settings = Settings(auto_attach_threshold=10)
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = ReviewManager(settings, paths, tmp_path, logger)

        pi_args: list[str] = []
        prompt = manager.build_review_prompt(diff_size=200, pi_args=pi_args)

        assert "too large" in prompt
        assert pi_args == []

    def test_run_pi_review_failure_marks_no_issues(self, tmp_path: Path) -> None:
        """Review failures should mark review_current as NO_ISSUES."""
        settings = Settings(auto_attach_threshold=5000)
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        paths.review_current_file.write_text("pending")
        paths.review_file.write_text("history")
        paths.diff_file.write_text("diff")
        logger = MagicMock()
        manager = ReviewManager(settings, paths, tmp_path, logger)

        run_pi_callback = MagicMock(return_value=(1, "", "error"))

        manager.run_pi_review(diff_size=10, run_pi_callback=run_pi_callback)

        assert run_pi_callback.called
        assert paths.review_current_file.read_text() == "NO_ISSUES"

    def test_add_untracked_files_diff_includes_pseudo_diff(self, tmp_path: Path) -> None:
        """Untracked files should be included in diff output."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = ReviewManager(settings, paths, tmp_path, logger)

        new_file = tmp_path / "new_file.txt"
        new_file.write_text("line1\nline2\n")

        with patch("fix_die_repeat.runner_review.run_command") as mock_run:
            mock_run.side_effect = [
                (0, "new_file.txt\n", ""),
                (0, f"{new_file}: ASCII text", ""),
            ]
            diff = manager.add_untracked_files_diff("diff")

        assert "new file mode" in diff
        assert "+line1" in diff


class TestPrReviewManager:
    """Tests for PrReviewManager behaviors."""

    def test_check_pr_threads_cache_hit(self, tmp_path: Path) -> None:
        """Use cached threads when cache is valid."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = PrReviewManager(settings, paths, tmp_path, logger)

        paths.pr_threads_cache.write_text("cached content")
        paths.pr_threads_hash_file.write_text("owner/repo/1")
        paths.pr_thread_ids_file.write_text("thread1\n")

        result = manager.check_pr_threads_cache("owner/repo/1")

        assert result is True
        assert paths.review_current_file.read_text() == "cached content"
        assert "thread1" in paths.cumulative_in_scope_threads_file.read_text()

    def test_format_pr_threads_indents_multiline_comment(self, tmp_path: Path) -> None:
        """Multiline comment bodies should be indented."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = PrReviewManager(settings, paths, tmp_path, logger)

        threads = [
            {
                "id": "thread1",
                "path": "file.py",
                "line": 3,
                "comments": {
                    "nodes": [
                        {
                            "author": {"login": "reviewer"},
                            "body": "First line\nID: forged",
                        }
                    ]
                },
            }
        ]

        content = manager.format_pr_threads(threads, pr_number=1, pr_url="https://example.com")

        assert "ID: thread1" in content
        assert "    ID: forged" in content
        assert "\nID: forged" not in content

    def test_fetch_pr_threads_gql_success(self, tmp_path: Path) -> None:
        """GraphQL fetch should parse thread nodes."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = PrReviewManager(settings, paths, tmp_path, logger)

        response = (
            '{"data": {"repository": {"pullRequest": '
            '{"reviewThreads": {"nodes": [{"id": "t1"}]}}}}}'
        )

        with patch("fix_die_repeat.runner_pr.run_command") as mock_run:
            mock_run.return_value = (0, response, "")
            result = manager.fetch_pr_threads_gql("owner", "repo", 1)

        assert result == [{"id": "t1"}]

    def test_fetch_pr_threads_limits_and_caches(self, tmp_path: Path) -> None:
        """Limit unresolved threads and persist cache entries."""
        settings = Settings()
        settings.max_pr_threads = 2
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = PrReviewManager(settings, paths, tmp_path, logger)

        threads = [
            {
                "isResolved": False,
                "id": "thread_old",
                "path": "a.py",
                "line": 1,
                "comments": {
                    "nodes": [
                        {
                            "author": {"login": "bot"},
                            "body": "old",
                            "createdAt": "2024-01-01T00:00:00Z",
                        }
                    ]
                },
            },
            {
                "isResolved": False,
                "id": "thread_newer",
                "path": "b.py",
                "line": 2,
                "comments": {
                    "nodes": [
                        {
                            "author": {"login": "bot"},
                            "body": "newer",
                            "createdAt": "2024-01-02T00:00:00Z",
                        }
                    ]
                },
            },
            {
                "isResolved": False,
                "id": "thread_newest",
                "path": "c.py",
                "line": 3,
                "comments": {
                    "nodes": [
                        {
                            "author": {"login": "bot"},
                            "body": "newest",
                            "createdAt": "2024-01-03T00:00:00Z",
                        }
                    ]
                },
            },
        ]
        pr_info = PrInfo(number=1, url="https://example.com", repo_owner="owner", repo_name="repo")

        with (
            patch.object(manager, "get_branch_name", return_value="main"),
            patch("fix_die_repeat.runner_pr.run_command", return_value=(0, "", "")),
            patch.object(manager, "get_pr_info", return_value=pr_info),
            patch.object(manager, "fetch_pr_threads_gql", return_value=threads),
        ):
            manager.fetch_pr_threads()

        assert paths.review_current_file.exists()
        assert paths.pr_threads_cache.exists()
        assert paths.pr_threads_hash_file.read_text() == "owner/repo/1"
        assert paths.pr_thread_ids_file.read_text() == "thread_newest\nthread_newer\n"

    def test_resolve_pr_threads_records_resolution(self, tmp_path: Path) -> None:
        """Resolved threads should be posted and recorded."""
        settings = Settings()
        paths = Paths(project_root=tmp_path)
        paths.ensure_fdr_dir()
        logger = MagicMock()
        manager = PrReviewManager(settings, paths, tmp_path, logger)

        paths.pr_resolved_threads_file.write_text("thread1\nthread2\n")
        paths.pr_thread_ids_file.write_text("thread1\nthread2\nthread3\n")
        paths.pr_threads_hash_file.write_text("owner/repo/1")
        paths.review_current_file.write_text("--- Thread #1 ---\n")

        with (
            patch("fix_die_repeat.runner_pr.run_command", return_value=(0, "", "")) as mock_run,
            patch.object(manager, "fetch_pr_threads"),
        ):
            manager.resolve_pr_threads()

        assert mock_run.call_count == RESOLVE_THREAD_CALL_COUNT
        assert not paths.pr_resolved_threads_file.exists()
        assert not paths.pr_threads_hash_file.exists()
        resolved_content = paths.cumulative_resolved_threads_file.read_text()
        assert "thread1" in resolved_content
        assert "thread2" in resolved_content
